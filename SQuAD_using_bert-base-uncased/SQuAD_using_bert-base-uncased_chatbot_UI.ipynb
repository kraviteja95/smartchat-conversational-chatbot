{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf35f622-e96b-45bb-8eda-aa81ac4f8faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "import gc\n",
    "\n",
    "# Create a context store\n",
    "context_store = []\n",
    "\n",
    "\n",
    "def update_context_store(context, question, answer):\n",
    "    \"\"\"\n",
    "    Update the context store with the provided context, question, and answer.\n",
    "\n",
    "    Args:\n",
    "        context (str): The context related to the question.\n",
    "        question (str): The question that was asked.\n",
    "        answer (str): The answer generated for the question.\n",
    "    \"\"\"\n",
    "    context_store.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "\n",
    "# Function to retrieve information from the context store\n",
    "def retrieve_from_context_store(question):\n",
    "    \"\"\"\n",
    "    Retrieve the context from the context store that matches the given question.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to search for in the context store.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The matching context if found, otherwise None.\n",
    "    \"\"\"\n",
    "    for item in context_store:\n",
    "        if question in item[\"question\"]:\n",
    "            return item[\"context\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to load the tokenizer and model\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"\n",
    "    Load the pre-trained BERT model and tokenizer for question answering.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the tokenizer, model, and device.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading model on device: {device}\")\n",
    "\n",
    "    model_save_path = \"squad-bert-trained/saved_model\"\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_save_path)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_save_path)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return tokenizer, model, device\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model, device = load_model_and_tokenizer()\n",
    "\n",
    "\n",
    "# Function to generate answers\n",
    "def generate_answer(context, question):\n",
    "    \"\"\"\n",
    "    Generate an answer for the given question based on the provided context.\n",
    "\n",
    "    Args:\n",
    "        context (str): The context related to the question.\n",
    "        question (str): The question to be answered.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer, or an error message if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        max_context_size = 512\n",
    "        chunk_size = max_context_size\n",
    "\n",
    "        chunks = [context[i:i + chunk_size] for i in range(0, len(context), chunk_size)]\n",
    "\n",
    "        answers = []\n",
    "        for chunk in chunks:\n",
    "            inputs = tokenizer(chunk, question, return_tensors='pt', truncation=True, max_length=max_context_size).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                answer_start_scores = outputs.start_logits\n",
    "                answer_end_scores = outputs.end_logits\n",
    "\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "                answers.append(answer)\n",
    "\n",
    "        answer = ' '.join(answers)\n",
    "        answer = answer.replace('[CLS]', '')\n",
    "\n",
    "        return answer.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        return \"‚ùå An error occurred while generating the answer.\"\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "def chatbot_interface():\n",
    "    \"\"\"\n",
    "    Create the Gradio interface for the chatbot.\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: A Gradio interface with context-aware chatbot functionality.\n",
    "    \"\"\"\n",
    "    with gr.Blocks() as demo:\n",
    "        # Adding custom CSS for beautifying the interface\n",
    "        gr.Markdown(\"\"\"\n",
    "            <style>\n",
    "                body {\n",
    "                    background-color: #f0f0f0;  /* Light gray background */\n",
    "                }\n",
    "                .chatbot-container {\n",
    "                    background-color: #ffffff;  /* White background for chatbot area */\n",
    "                    border-radius: 10px;\n",
    "                    padding: 20px;\n",
    "                    color: #333;  /* Dark text color */\n",
    "                    font-family: Arial, sans-serif;\n",
    "                }\n",
    "                .gr-button {\n",
    "                    background-color: #4CAF50;  /* Green button */\n",
    "                    color: white;\n",
    "                    border: none;\n",
    "                    border-radius: 5px;\n",
    "                    padding: 10px 20px;\n",
    "                    font-size: 14px;\n",
    "                    cursor: pointer;\n",
    "                }\n",
    "                .gr-button:hover {\n",
    "                    background-color: #45a049;  /* Darker green on hover */\n",
    "                }\n",
    "                .gr-textbox {\n",
    "                    background-color: #ffffff;  /* White background for textboxes */\n",
    "                    color: #333;  /* Dark text color in textbox */\n",
    "                    border-radius: 5px;\n",
    "                    border: 1px solid #ddd;\n",
    "                    padding: 10px;\n",
    "                }\n",
    "                .gr-chatbot {\n",
    "                    background-color: #e6e6e6;  /* Light gray background for chatbot */\n",
    "                    border-radius: 10px;\n",
    "                    padding: 15px;\n",
    "                    color: #333;\n",
    "                }\n",
    "                .status-message {\n",
    "                    color: #007bff;  /* Blue status message */\n",
    "                    font-weight: bold;\n",
    "                }\n",
    "                .footer {\n",
    "                    text-align: right;\n",
    "                    font-size: 12px;\n",
    "                    color: #777;\n",
    "                    font-style: italic;\n",
    "                }\n",
    "            </style>\n",
    "        \"\"\")\n",
    "\n",
    "        gr.Markdown(\"<h1 style='text-align: center; color: #4CAF50;'>üß† SmartChat: A Context-Aware Conversational Agent</h1>\")\n",
    "        gr.Markdown(\"<p style='text-align: center; color: #777;'>Set a context and then ask multiple questions based on that context.</p>\")\n",
    "\n",
    "        context_state = gr.State()\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                context_input = gr.Textbox(\n",
    "                    label=\"Set Context\",\n",
    "                    placeholder=\"Enter the context here...\",\n",
    "                    lines=4\n",
    "                )\n",
    "                set_context_btn = gr.Button(\"Set Context\")\n",
    "\n",
    "                clear_context_btn = gr.Button(\"Clear Context\")\n",
    "\n",
    "                status_message = gr.Markdown(\"\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(label=\"Chatbot\")\n",
    "\n",
    "        question_input = gr.Textbox(\n",
    "            label=\"Ask a Question\",\n",
    "            placeholder=\"Enter your question here...\",\n",
    "            lines=1\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Submit Question\")\n",
    "\n",
    "        footer = gr.Markdown(\"\"\"\n",
    "            <div style='display: flex; justify-content: space-between; font-size: 12px; color: #777;'>\n",
    "                <p style='margin: 0;'>Trained using: bert-base-uncased</p>\n",
    "                <p style='margin: 0;'>Prepared by: Ravi Teja Kothuru, Soumi Ray and Anwesha Sarangi</p>\n",
    "            </div>\n",
    "        \"\"\")\n",
    "\n",
    "        def set_context(context):\n",
    "            \"\"\"\n",
    "            Set the provided context for future question-answering.\n",
    "\n",
    "            Args:\n",
    "                context (str): The context to set.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple of updated UI components after setting the context.\n",
    "            \"\"\"\n",
    "            if not context.strip():\n",
    "                return gr.update(), \"Please enter a valid context.\", None\n",
    "            return gr.update(visible=False), \"Context has been set. You can now ask questions.\", context\n",
    "\n",
    "        def clear_context():\n",
    "            \"\"\"\n",
    "            Clear the current context.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple of updated UI components after clearing the context.\n",
    "            \"\"\"\n",
    "            return gr.update(visible=True), \"Context has been cleared. Please set a new context.\", None\n",
    "\n",
    "        def handle_question(question, history, context):\n",
    "            \"\"\"\n",
    "            Handle the question by generating an answer based on the context.\n",
    "\n",
    "            Args:\n",
    "                question (str): The question to answer.\n",
    "                history (list): The conversation history.\n",
    "                context (str): The context for generating the answer.\n",
    "\n",
    "            Returns:\n",
    "                tuple: Updated conversation history and the cleared question input.\n",
    "            \"\"\"\n",
    "            if not context:\n",
    "                return history, \"Please set the context before asking questions.\"\n",
    "            if not question.strip():\n",
    "                return history, \"Please enter a valid question.\"\n",
    "\n",
    "            answer = generate_answer(context, question)\n",
    "            history = history + [[f\"üë§: {question}\", f\"ü§ñ: {answer}\"]]\n",
    "            return history, \"\"\n",
    "\n",
    "        set_context_btn.click(set_context, inputs=context_input, outputs=[context_input, status_message, context_state])\n",
    "        clear_context_btn.click(clear_context, inputs=None, outputs=[context_input, status_message, context_state])\n",
    "        submit_btn.click(\n",
    "            handle_question,\n",
    "            inputs=[question_input, chatbot, context_state],\n",
    "            outputs=[chatbot, question_input]\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo = chatbot_interface()\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565b0be-bfc1-4574-9d27-abf12f6d8b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
