{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09a34b1-f0a1-4db2-9751-76e409da0943",
   "metadata": {},
   "source": [
    "**Authors:**\n",
    "\n",
    "- Ravi Teja Kothuru (Primary)\n",
    "- Soumi Ray\n",
    "- Anwesha Sarangi\n",
    "\n",
    "**Title of the Project:** SmartChat: A Context-Aware Conversational Agent\n",
    "\n",
    "**Description of the Project:** Develop a chatbot that can effectively adapt to context and topic shifts in a conversation, leveraging the Stanford Question Answering Dataset to provide informed and relevant responses, and thereby increasing user satisfaction and engagement.\n",
    "\n",
    "**Objectives of the Project:** Create a user-friendly web or app interface that enables users to have natural and coherent conversations with the chatbot, with high satisfaction rating.\n",
    "\n",
    "**Name of the Dataset:** Stanford Question Answering Dataset\n",
    "\n",
    "**Description of the Dataset:** The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text, or span, from the corresponding reading passage. There are 100,000+ question-answer pairs on 500+ articles. More information can be found at: https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "**Dataset Source:**\n",
    "\n",
    "Kaggle (https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset)\n",
    "\n",
    "***Number of Variables in Dataset:*** There are 2 variables in this dataset\n",
    "\n",
    "- data\n",
    "- version\n",
    "\n",
    "Each of these have other variables such as:\n",
    "\n",
    "- ***context:*** A lengthy paragraph that has some information.\n",
    "- ***question:*** A question based on the context.\n",
    "- ***answer:*** An answer to the context from the context.\n",
    "- ***ans_start:*** The index value of context where the answer to the question is started.\n",
    "- ***ans_end:*** The index value of context where the answer to the question is ended.\n",
    "\n",
    "***Size of the Dataset:*** The dataset has 2 JSON files. One is for training and the other is for validation\n",
    "\n",
    "- Training Dataset's filename is train-v1.1.json and it size is 30.3 MB.\n",
    "- Validation Dataset's filename is dev-v1.1.json and it size is 4.9 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaafc742-c8ac-480f-bc70-d55f783bb61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (3.6.2)\n",
      "Requirement already satisfied: fastapi<1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.115.2)\n",
      "Requirement already satisfied: ffmpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (2.1.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (3.10.6)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (23.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (10.0.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.6.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (0.18.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.40.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.15.4)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.3.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.14)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Loading model on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gradio/components/chatbot.py:222: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "!pip install gradio\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "import gc\n",
    "\n",
    "\n",
    "# Function to load the tokenizer and model\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    \"\"\"\n",
    "    Loads the tokenizer and the fine-tuned LoRA model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the fine-tuned LoRA model.\n",
    "\n",
    "    Returns:\n",
    "        tokenizer, model, device\n",
    "    \"\"\"\n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Loading model on device: {device}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-medium\")\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Load fine-tuned LoRA model\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return tokenizer, model, device\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer, model, device = load_model_and_tokenizer(\"gpt2-medium-lora\")\n",
    "\n",
    "\n",
    "# Function to generate answers\n",
    "def generate_answer(context, question):\n",
    "    \"\"\"\n",
    "    Generates an answer based on the provided context and question.\n",
    "\n",
    "    Args:\n",
    "        context (str): The context for the questions.\n",
    "        question (str): The user's question.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define the prompt format as per training\n",
    "        prompt = f\"Context: {context}\\nQuestion: {question}\\n <|start_answer|>\"\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = tokenizer(prompt, padding=\"max_length\", truncation=True, max_length=800, return_tensors='pt')\n",
    "\n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=32,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                temperature=0.2,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "        # Decode the output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract the answer\n",
    "        start_token = \"<|start_answer|>\"\n",
    "        start_idx = generated_text.find(start_token) + len(start_token)\n",
    "        if start_idx != -1:\n",
    "            generated_text = generated_text[start_idx:].strip()\n",
    "\n",
    "        question_index = generated_text.find(\"Question:\")\n",
    "        \n",
    "        if question_index != -1:\n",
    "            generated_text = generated_text[0:question_index].strip()\n",
    "        return generated_index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during generation: {e}\")\n",
    "        return \"❌ An error occurred while generating the answer.\"\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "def chatbot_interface():\n",
    "    \"\"\"\n",
    "    Create the Gradio interface for the chatbot.\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: A Gradio interface with context-aware chatbot functionality.\n",
    "    \"\"\"\n",
    "    with gr.Blocks() as demo:\n",
    "        # Adding custom CSS for beautifying the interface\n",
    "        gr.Markdown(\"\"\"\n",
    "            <style>\n",
    "                body {\n",
    "                    background-color: #f0f0f0;  /* Light gray background */\n",
    "                }\n",
    "                .chatbot-container {\n",
    "                    background-color: #ffffff;  /* White background for chatbot area */\n",
    "                    border-radius: 10px;\n",
    "                    padding: 20px;\n",
    "                    color: #333;  /* Dark text color */\n",
    "                    font-family: Arial, sans-serif;\n",
    "                }\n",
    "                .gr-button {\n",
    "                    background-color: #4CAF50;  /* Green button */\n",
    "                    color: white;\n",
    "                    border: none;\n",
    "                    border-radius: 5px;\n",
    "                    padding: 10px 20px;\n",
    "                    font-size: 14px;\n",
    "                    cursor: pointer;\n",
    "                }\n",
    "                .gr-button:hover {\n",
    "                    background-color: #45a049;  /* Darker green on hover */\n",
    "                }\n",
    "                .gr-textbox {\n",
    "                    background-color: #ffffff;  /* White background for textboxes */\n",
    "                    color: #333;  /* Dark text color in textbox */\n",
    "                    border-radius: 5px;\n",
    "                    border: 1px solid #ddd;\n",
    "                    padding: 10px;\n",
    "                }\n",
    "                .gr-chatbot {\n",
    "                    background-color: #e6e6e6;  /* Light gray background for chatbot */\n",
    "                    border-radius: 10px;\n",
    "                    padding: 15px;\n",
    "                    color: #333;\n",
    "                }\n",
    "                .status-message {\n",
    "                    color: #007bff;  /* Blue status message */\n",
    "                    font-weight: bold;\n",
    "                }\n",
    "                .footer {\n",
    "                    text-align: right;\n",
    "                    font-size: 12px;\n",
    "                    color: #777;\n",
    "                    font-style: italic;\n",
    "                }\n",
    "            </style>\n",
    "        \"\"\")\n",
    "\n",
    "        # State to store context\n",
    "        gr.Markdown(\"<h1 style='text-align: center; color: #4CAF50;'>🧠 SmartChat: A Context-Aware Conversational Agent</h1>\")\n",
    "        gr.Markdown(\"<p style='text-align: center; color: #777;'>Set a context and then ask multiple questions based on that context.</p>\")\n",
    "        context_state = gr.State()\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                # Context input\n",
    "                context_input = gr.Textbox(\n",
    "                    label=\"Set Context\",\n",
    "                    placeholder=\"Enter the context here...\",\n",
    "                    lines=4\n",
    "                )\n",
    "                set_context_btn = gr.Button(\"Set Context\")\n",
    "\n",
    "                # Clear Context button\n",
    "                clear_context_btn = gr.Button(\"Clear Context\")\n",
    "\n",
    "                # Status message\n",
    "                status_message = gr.Markdown(\"\")\n",
    "\n",
    "            with gr.Column(scale=2):\n",
    "                # Chatbot display\n",
    "                chatbot = gr.Chatbot(label=\"Chatbot\")\n",
    "\n",
    "        # Question input\n",
    "        question_input = gr.Textbox(\n",
    "            label=\"Ask a Question\",\n",
    "            placeholder=\"Enter your question here...\",\n",
    "            lines=1\n",
    "        )\n",
    "        submit_btn = gr.Button(\"Submit Question\")\n",
    "\n",
    "        footer = gr.Markdown(\"\"\"\n",
    "            <div style='display: flex; justify-content: space-between; font-size: 12px; color: #777;'>\n",
    "                <p style='margin: 0;'>Trained using: GPT2-Medium LORA</p>\n",
    "                <p style='margin: 0;'>Prepared by: Ravi Teja Kothuru, Soumi Ray and Anwesha Sarangi</p>\n",
    "            </div>\n",
    "        \"\"\")\n",
    "\n",
    "        # Function to set context\n",
    "        def set_context(context):\n",
    "            \"\"\"\n",
    "            Set the provided context for future question-answering.\n",
    "\n",
    "            Args:\n",
    "                context (str): The context to set.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple of updated UI components after setting the context.\n",
    "            \"\"\"\n",
    "            if not context.strip():\n",
    "                return gr.update(), \"Please enter a valid context.\", None\n",
    "            return gr.update(visible=False), \"Context has been set. You can now ask questions.\", context\n",
    "\n",
    "        # Function to clear context\n",
    "        def clear_context():\n",
    "            \"\"\"\n",
    "            Clear the current context.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple of updated UI components after clearing the context.\n",
    "            \"\"\"\n",
    "            return gr.update(visible=True), \"Context has been cleared. Please set a new context.\", None\n",
    "\n",
    "        # Function to handle question submission\n",
    "        def handle_question(question, history, context):\n",
    "            \"\"\"\n",
    "            Handle the question by generating an answer based on the context.\n",
    "\n",
    "            Args:\n",
    "                question (str): The question to answer.\n",
    "                history (list): The conversation history.\n",
    "                context (str): The context for generating the answer.\n",
    "\n",
    "            Returns:\n",
    "                tuple: Updated conversation history and the cleared question input.\n",
    "            \"\"\"\n",
    "            if not context:\n",
    "                return history, \"Please set the context before asking questions.\"\n",
    "            if not question.strip():\n",
    "                return history, \"Please enter a valid question.\"\n",
    "\n",
    "            answer = generate_answer(context, question)\n",
    "            history = history + [[f\"👤 : {question}\", f\"🤖 : {answer}\"]]\n",
    "            return history, \"\"\n",
    "\n",
    "        # Event bindings\n",
    "        set_context_btn.click(set_context, inputs=context_input, outputs=[context_input, status_message, context_state])\n",
    "        clear_context_btn.click(clear_context, inputs=None, outputs=[context_input, status_message, context_state])\n",
    "        submit_btn.click(\n",
    "            handle_question,\n",
    "            inputs=[question_input, chatbot, context_state],\n",
    "            outputs=[chatbot, question_input]\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "# Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    demo = chatbot_interface()\n",
    "    demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7458e2-9b72-42b8-973b-e3cc65a51130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
