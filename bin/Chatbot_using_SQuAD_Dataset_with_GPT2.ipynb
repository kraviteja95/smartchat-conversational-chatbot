{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntHZJ9nIAgkp",
    "outputId": "dda73bfc-b678-4586-a39a-5ff3ec3408a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: torchaudio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (3.10.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets torch torchvision torchaudio\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of PyTorch detecting the Metal device in MacBook:\n",
    "\n",
    "This is useful to check if GPU is available in Macbook. If available, then we can make use of it to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MacBook has metal available. We can use GPU to train the model\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    print(\"MacBook has metal available. We can use GPU to train the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_common_path = \"/Users/ravkothu/Documents/Personal_items_at_Oracle/Master_Degree/University_of_San_Diego/Online_Masters/MS_in_Applied_AI/Subjects_and_Resources/AAI-520-A2_NLP/AAI-520-A2_Final_Team_Project/NLP_Datasets\"\n",
    "split1_train_df_path = f\"{dataset_common_path}/train_dataset_split_1.csv\"\n",
    "\n",
    "# Read first part train dataset into a dataframe\n",
    "split1_train_df = pd.read_csv(split1_train_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21900, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21900 entries, 0 to 21899\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           21900 non-null  object\n",
      " 1   title        21900 non-null  object\n",
      " 2   input_text   21900 non-null  object\n",
      " 3   target_text  21900 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 684.5+ KB\n"
     ]
    }
   ],
   "source": [
    "split1_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>['Question: To whom did the Virgin Mary allege...</td>\n",
       "      <td>['Saint Bernadette Soubirous']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>['Question: What is in front of the Notre Dame...</td>\n",
       "      <td>['a copper statue of Christ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>['Question: The Basilica of the Sacred heart a...</td>\n",
       "      <td>['the Main Building']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>['Question: What is the Grotto at Notre Dame? ...</td>\n",
       "      <td>['a Marian place of prayer and reflection']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>['Question: What sits on top of the Main Build...</td>\n",
       "      <td>['a golden statue of the Virgin Mary']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id                     title  \\\n",
       "0  5733be284776f41900661182  University_of_Notre_Dame   \n",
       "1  5733be284776f4190066117f  University_of_Notre_Dame   \n",
       "2  5733be284776f41900661180  University_of_Notre_Dame   \n",
       "3  5733be284776f41900661181  University_of_Notre_Dame   \n",
       "4  5733be284776f4190066117e  University_of_Notre_Dame   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  ['Question: To whom did the Virgin Mary allege...   \n",
       "1  ['Question: What is in front of the Notre Dame...   \n",
       "2  ['Question: The Basilica of the Sacred heart a...   \n",
       "3  ['Question: What is the Grotto at Notre Dame? ...   \n",
       "4  ['Question: What sits on top of the Main Build...   \n",
       "\n",
       "                                   target_text  \n",
       "0               ['Saint Bernadette Soubirous']  \n",
       "1                ['a copper statue of Christ']  \n",
       "2                        ['the Main Building']  \n",
       "3  ['a Marian place of prayer and reflection']  \n",
       "4       ['a golden statue of the Virgin Mary']  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21895</th>\n",
       "      <td>56f8da749b226e1400dd10f3</td>\n",
       "      <td>Near_East</td>\n",
       "      <td>['Question: Until what year did the Ottomans r...</td>\n",
       "      <td>['1912']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21896</th>\n",
       "      <td>56f8da749b226e1400dd10f4</td>\n",
       "      <td>Near_East</td>\n",
       "      <td>['Question: When did the Ottomans lose the ter...</td>\n",
       "      <td>['the two Balkan Wars of 1912–13']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21897</th>\n",
       "      <td>56f8dbf69b226e1400dd1118</td>\n",
       "      <td>Near_East</td>\n",
       "      <td>['Question: How was the Ottoman Empire portray...</td>\n",
       "      <td>['as the sick man of Europe']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21898</th>\n",
       "      <td>56f8dbf69b226e1400dd1119</td>\n",
       "      <td>Near_East</td>\n",
       "      <td>['Question: The Balkan states were primarily w...</td>\n",
       "      <td>['Christian']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21899</th>\n",
       "      <td>56f8dbf69b226e1400dd111a</td>\n",
       "      <td>Near_East</td>\n",
       "      <td>['Question: When did the Ottomans strike at th...</td>\n",
       "      <td>['1894']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id      title  \\\n",
       "21895  56f8da749b226e1400dd10f3  Near_East   \n",
       "21896  56f8da749b226e1400dd10f4  Near_East   \n",
       "21897  56f8dbf69b226e1400dd1118  Near_East   \n",
       "21898  56f8dbf69b226e1400dd1119  Near_East   \n",
       "21899  56f8dbf69b226e1400dd111a  Near_East   \n",
       "\n",
       "                                              input_text  \\\n",
       "21895  ['Question: Until what year did the Ottomans r...   \n",
       "21896  ['Question: When did the Ottomans lose the ter...   \n",
       "21897  ['Question: How was the Ottoman Empire portray...   \n",
       "21898  ['Question: The Balkan states were primarily w...   \n",
       "21899  ['Question: When did the Ottomans strike at th...   \n",
       "\n",
       "                              target_text  \n",
       "21895                            ['1912']  \n",
       "21896  ['the two Balkan Wars of 1912–13']  \n",
       "21897       ['as the sick man of Europe']  \n",
       "21898                       ['Christian']  \n",
       "21899                            ['1894']  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1_train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>21900</td>\n",
       "      <td>21900</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>21900</td>\n",
       "      <td>97</td>\n",
       "      <td>New_York_City</td>\n",
       "      <td>817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input_text</th>\n",
       "      <td>21900</td>\n",
       "      <td>21856</td>\n",
       "      <td>['Question: Who was a pop idol that started on...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_text</th>\n",
       "      <td>21900</td>\n",
       "      <td>16283</td>\n",
       "      <td>['three']</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count unique                                                top  \\\n",
       "id           21900  21900                           5733be284776f41900661182   \n",
       "title        21900     97                                      New_York_City   \n",
       "input_text   21900  21856  ['Question: Who was a pop idol that started on...   \n",
       "target_text  21900  16283                                          ['three']   \n",
       "\n",
       "            freq  \n",
       "id             1  \n",
       "title        817  \n",
       "input_text     5  \n",
       "target_text   68  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1_train_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "title          0\n",
       "input_text     0\n",
       "target_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split1_train_df_missing_values = split1_train_df.isnull().sum()\n",
    "split1_train_df_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 5733be284776f41900661182\n",
      "\n",
      "Title: University_of_Notre_Dame\n",
      "\n",
      "Input Text: ['Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? Context: Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.']\n",
      "\n",
      "Target Text: ['Saint Bernadette Soubirous']\n"
     ]
    }
   ],
   "source": [
    "# Access the first row\n",
    "first_row = split1_train_df.iloc[0]\n",
    "\n",
    "# Print each value separately\n",
    "print(f\"ID: {first_row['id']}\\n\")\n",
    "print(f\"Title: {first_row['title']}\\n\")\n",
    "print(f\"Input Text: {first_row['input_text']}\\n\")\n",
    "print(f\"Target Text: {first_row['target_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                      5733be284776f41900661182\n",
       "title                                   University_of_Notre_Dame\n",
       "input_text     ['Question: To whom did the Virgin Mary allege...\n",
       "target_text                       ['Saint Bernadette Soubirous']\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of each class in 'class_column':\n",
      "                                                title  count\n",
      "0                                       New_York_City    817\n",
      "1                                       American_Idol    802\n",
      "2                                             Beyoncé    758\n",
      "3                                     Frédéric_Chopin    697\n",
      "4                                            Buddhism    610\n",
      "5                             2008_Sichuan_earthquake    521\n",
      "6                    2008_Summer_Olympics_torch_relay    500\n",
      "7                                            Portugal    435\n",
      "8                                          Kanye_West    428\n",
      "9                                         Southampton    426\n",
      "10                                                Dog    392\n",
      "11                Financial_crisis_of_2007%E2%80%9308    390\n",
      "12                                                Pub    377\n",
      "13                                           Plymouth    367\n",
      "14                                             Boston    362\n",
      "15                                               Alps    346\n",
      "16                                   Catalan_language    341\n",
      "17                                               IPod    327\n",
      "18                                    Josip_Broz_Tito    326\n",
      "19                              Alexander_Graham_Bell    325\n",
      "20                             Alfred_North_Whitehead    317\n",
      "21                                            Nanjing    317\n",
      "22                                    Classical_music    314\n",
      "23                                       Saint_Helena    313\n",
      "24     Sino-Tibetan_relations_during_the_Ming_dynasty    291\n",
      "25                                             Treaty    281\n",
      "26                                               Gene    279\n",
      "27                                   Marshall_Islands    277\n",
      "28                                           Szlachta    276\n",
      "29                                Spectre_(2015_film)    274\n",
      "30                               Daylight_saving_time    273\n",
      "31                              Arena_Football_League    272\n",
      "32                           University_of_Notre_Dame    269\n",
      "33                                       Solar_energy    250\n",
      "34                                      Guinea-Bissau    243\n",
      "35                              To_Kill_a_Mockingbird    228\n",
      "36                                       Anthropology    222\n",
      "37                                              Brain    222\n",
      "38                              Arnold_Schwarzenegger    221\n",
      "39                           Adult_contemporary_music    216\n",
      "40             The_Legend_of_Zelda:_Twilight_Princess    214\n",
      "41              List_of_numbered_streets_in_Manhattan    213\n",
      "42                                              Slavs    206\n",
      "43                                  Westminster_Abbey    202\n",
      "44                                  Universal_Studios    201\n",
      "45                               Political_corruption    195\n",
      "46                                         Space_Race    186\n",
      "47                                       Architecture    185\n",
      "48                                               Bern    181\n",
      "49                              Canadian_Armed_Forces    179\n",
      "50                                      Oklahoma_City    179\n",
      "51       Russian_Soviet_Federative_Socialist_Republic    175\n",
      "52                                           Hydrogen    174\n",
      "53                                        Antibiotics    169\n",
      "54                                            Dialect    165\n",
      "55                               University_of_Kansas    161\n",
      "56                                         Tajikistan    154\n",
      "57                            Institute_of_technology    154\n",
      "58                                            Montana    150\n",
      "59                              Republic_of_the_Congo    145\n",
      "60                 BeiDou_Navigation_Satellite_System    144\n",
      "61                                    Hunter-gatherer    144\n",
      "62                                           Lighting    141\n",
      "63                                             Comics    140\n",
      "64                                           Genocide    137\n",
      "65                                  Estonian_language    130\n",
      "66                              Saint_Barth%C3%A9lemy    120\n",
      "67              Royal_Institute_of_British_Architects    120\n",
      "68                             Cardinal_(Catholicism)    117\n",
      "69                          Communications_in_Somalia    112\n",
      "70                                     BBC_Television    111\n",
      "71                                              Paper    108\n",
      "72       National_Archives_and_Records_Administration    106\n",
      "73                                          Canon_law    105\n",
      "74                                     Prime_minister    100\n",
      "75                     United_Nations_Population_Fund     99\n",
      "76                                             Virgil     96\n",
      "77                                             Heresy     93\n",
      "78                                Aspirated_consonant     90\n",
      "79                                    Southern_Europe     89\n",
      "80  Separation_of_powers_under_the_United_States_C...     88\n",
      "81                           Sony_Music_Entertainment     85\n",
      "82                                          Christian     84\n",
      "83                            Human_Development_Index     82\n",
      "84                          Internet_service_provider     82\n",
      "85                                    Wayback_Machine     79\n",
      "86                                             Genome     76\n",
      "87                               Comprehensive_school     73\n",
      "88                                        Web_browser     72\n",
      "89                                     Dutch_Republic     68\n",
      "90                                          Symbiosis     65\n",
      "91                                  Iranian_languages     65\n",
      "92                                        Materialism     54\n",
      "93                                        Warsaw_Pact     52\n",
      "94                                   Tristan_da_Cunha     44\n",
      "95                                             Matter     24\n",
      "96                                          Near_East     15\n"
     ]
    }
   ],
   "source": [
    "# Check the count of each class in the 'title'\n",
    "class_counts = split1_train_df['title'].value_counts()\n",
    "\n",
    "class_counts_df = class_counts.reset_index()\n",
    "# Rename columns for clarity\n",
    "class_counts_df.columns = ['title', 'count']\n",
    "\n",
    "# Print the DataFrame with class counts\n",
    "print(\"Count of each class in 'class_column':\")\n",
    "print(class_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decide which architecture to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of the architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop chatbots, we have different architectures. \n",
    "Let us better understand about them before deciding which one to use.\n",
    "\n",
    "# Differences Between Seq2Seq, Transformers, and GPT\n",
    "\n",
    "| Feature       | Seq2Seq                                           | Transformers                                    | GPT                                               |\n",
    "|---------------|--------------------------------------------------|------------------------------------------------|--------------------------------------------------|\n",
    "| **Definition**| A model that transforms an input sequence into an output sequence using an encoder and decoder. | A deep learning architecture using self-attention mechanisms to process input sequences. | A specific Transformer model designed for generating text by predicting the next word in a sequence. |\n",
    "| **Usage**     | Tasks where input and output are sequences, like translation and summarization. | A wide range of NLP tasks, including translation and summarization. | Primarily used for text generation tasks like chatbots and text completion. |\n",
    "| **Information**| Consists of an encoder that processes the input and a decoder that generates the output. | Composed of an encoder and decoder stack, using self-attention to capture relationships between words. | Utilizes only the decoder part of the Transformer, focusing on unidirectional text generation. |\n",
    "| **Strengths** | Effective for varying output lengths; good at capturing context. | Can process sequences in parallel; captures long-range dependencies well. | Excellent at generating coherent and contextually relevant text; adapts to various topics. |\n",
    "| **Limitations**| Struggles with long sequences due to fixed-length context vectors; may not capture long-range dependencies well. | Requires substantial data and computational power; complexity can make fine-tuning harder. | May generate repetitive or nonsensical outputs; unidirectional nature limits contextual understanding compared to bidirectional models. |\n",
    "| **Applications**| Machine translation, text summarization, conversational agents. | Machine translation, text generation, sentiment analysis. | Chatbots, text completion, creative writing assistance. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Decision of the Architecture to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I will be going with GPT architecture. \n",
    "\n",
    "Following are the reasons for the same:\n",
    "\n",
    "- **Text Generation Capability:** GPT has been specifically designed for generating the text, making it highly effective for producing logically reasoned responses in conversational contexts such as to-and-fro chats with chatbot.\n",
    "\n",
    "- **Availability of Pre-trained Models:** Models like GPT-3 are pre-trained on extensive datasets. These kind of models deliver contextually appropriate responses on a wide range of topics without the need for extensive additional training.\n",
    "\n",
    "- **Adaptability and Versatility:** The GPT architecture's exposure to a variety of text sources, allows it to adjust to different conversational styles, thereby imprvoing the user interactions.\n",
    "\n",
    "- **Contextual Awareness:** GPT is very good at maintaining context throughout multiple chat exchanges. It leads to smoother and more continuous dialogues.\n",
    "\n",
    "- **Implementation and Fine-tuning:** Making use of a pre-trained GPT model, we can conserve time and resources, as it demands less data and training compared to Seq2Seq models.\n",
    "\n",
    "- **Performance in Chatbot Applications:** GPT has shown exceptional capability in generating human-like responses.\n",
    "\n",
    "Considering all these points, we have decided to go with the GPT architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training using GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually for these kind of usecases, model training involves so many steps.\n",
    "Look at the following steps for the same:\n",
    "\n",
    "- Load the `split1_train_df` dataframe.\n",
    "- Here, there is no use of converting the rows of the dataframe into different formats. All the rows are already present in proper structure. \n",
    "- Hence, we can directly go ahead with splitting the dataframe to training (80%) and testing (20%) datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataframe into train (80%) and test (20%) datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 17520\n",
      "Testing set size: 4380\n"
     ]
    }
   ],
   "source": [
    "splitted_train_df, splitted_test_df = train_test_split(\n",
    "    split1_train_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Display the sizes of the train and test sets\n",
    "print(f\"Training set size: {len(splitted_train_df)}\")\n",
    "print(f\"Testing set size: {len(splitted_test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization using GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenize the both `splitted_train_df` and `splitted_test_df` using `gpt2` model's `GPT2Tokenizer` tokenizer.\n",
    "    - ***Pass the following custom parameters to tokenize:***\n",
    "        - In models like GPT-2, padding is required to make sure that all the sequences in a batch are having same length. Hence, by setting the padding token to the `eos_token`, we make the data compatible with the model's requirements and make sure that the model understands that these tokens are simply placeholders for the space that was in shorter sequences.\n",
    "        - ***Define the tokenizing actual function having the following details.***\n",
    "            - Convert the input text column of the DataFrame into a list of strings, which will be tokenized.\n",
    "            - `truncation=True` to be sure that if the input text exceeds the specified maximum length, it will be truncated to fit\n",
    "            - `padding=True` to be sure that all sequences in the output are padded to the length of the longest sequence in the batch, so they all have the same length.\n",
    "            - Return the tokenized output as PyTorch tensors, which can be used directly for training with PyTorch using `return_tensors='pt'`.\n",
    "            - Set the maximum length of the tokenized sequences to `512`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display the shapes after tokenization\n",
      "*************************************\n",
      "Training Input Encoding Shape: torch.Size([17520, 512])\n",
      "Training Target Encoding Shape: torch.Size([17520, 512])\n",
      "Testing Input Encoding Shape: torch.Size([4380, 512])\n",
      "Testing Target Encoding Shape: torch.Size([4380, 512])\n"
     ]
    }
   ],
   "source": [
    "# Set the padding token to the end-of-sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define and write the logic to tokenize the data\n",
    "def tokenize_data(df):\n",
    "    # Tokenize input texts and target texts\n",
    "    input_encodings = tokenizer(\n",
    "        df['input_text'].tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    target_encodings = tokenizer(\n",
    "        df['target_text'].tolist(),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt',\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    return input_encodings, target_encodings\n",
    "\n",
    "# Tokenize the training data\n",
    "train_input_encodings, train_target_encodings = tokenize_data(splitted_train_df)\n",
    "\n",
    "# Tokenize the testing data\n",
    "test_input_encodings, test_target_encodings = tokenize_data(splitted_test_df)\n",
    "\n",
    "# Display shapes of the encodings\n",
    "print(\"Display the shapes after tokenization\")\n",
    "print(\"*************************************\")\n",
    "print(f\"Training Input Encoding Shape: {train_input_encodings['input_ids'].shape}\")\n",
    "print(f\"Training Target Encoding Shape: {train_target_encodings['input_ids'].shape}\")\n",
    "print(f\"Testing Input Encoding Shape: {test_input_encodings['input_ids'].shape}\")\n",
    "print(f\"Testing Target Encoding Shape: {test_target_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Objects and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we should create custom dataset objects for training and testing using PyTorch's Dataset class. This is usually done to allow us to handle batching and shuffling during training.\n",
    "- We should use PyTorch's DataLoader to create iterable batches from our dataset objects. This makes easy to load data during training and it can also include options for shuffling and parallel loading.\n",
    "\n",
    "Let us understand better about the same:\n",
    "\n",
    "- ***Custom Dataset Class Creation:***\n",
    "    - The `GPT2Dataset` class inherits from `torch.utils.data.Dataset`.\n",
    "    - The constructor is to take the input and target encodings and stores them.\n",
    "    - The `__len__` method returns the total number of samples.\n",
    "    - The `__getitem__` method retrieves the input and target data for a specific index.\n",
    "\n",
    "- ***Dataset Objects Creation:***\n",
    "    - Instances of `GPT2Dataset` are created for both the training and testing datasets.\n",
    "    \n",
    "- ***DataLoader Objects Creation:***\n",
    "    - `DataLoader` is used to create iterable batches from the datasets.\n",
    "    - We have the facility to adjust the `batch_size` parameter as needed. I have set to 8 for the time being.\n",
    "    - The training `DataLoader` is set to shuffle the data, while the test `DataLoader` is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 17520\n",
      "Testing dataset size: 4380\n",
      "Example batch input IDs shape: torch.Size([8, 512])\n",
      "Example batch target IDs shape: torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "# Custom Dataset class that inherits from PyTorch's Dataset class\n",
    "class GPT2Dataset(Dataset):\n",
    "    # Constructor to initialize the dataset object\n",
    "    def __init__(self, input_encodings, target_encodings):\n",
    "        # Store the input_ids from the input_encodings dictionary\n",
    "        self.input_ids = input_encodings['input_ids']\n",
    "        # Store the attention_mask from the input_encodings dictionary\n",
    "        self.attention_mask = input_encodings['attention_mask']\n",
    "        # Store the target_ids from the target_encodings dictionary\n",
    "        self.target_ids = target_encodings['input_ids']\n",
    "\n",
    "    # Get the size of the dataset\n",
    "    def __len__(self):\n",
    "        # Return the total number of input samples (input_ids)\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # Method to get a specific sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a dictionary containing input_ids, attention_mask, and target_ids for the specified index\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],  # Get input IDs for the specified index\n",
    "            'attention_mask': self.attention_mask[idx],  # Get attention mask for the specified index\n",
    "            'target_ids': self.target_ids[idx],  # Get target IDs for the specified index\n",
    "        }\n",
    "\n",
    "# Create Dataset objects for training and testing datasets\n",
    "# Create training dataset\n",
    "train_dataset = GPT2Dataset(train_input_encodings, train_target_encodings)\n",
    "\n",
    "# Create testing dataset\n",
    "test_dataset = GPT2Dataset(test_input_encodings, test_target_encodings)\n",
    "\n",
    "# Create DataLoader objects for both datasets to facilitate batch processing\n",
    "# Create DataLoader for training with shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Create DataLoader for testing without shuffling\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Display the size of the datasets\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Get an example batch from the training DataLoader\n",
    "# Retrieve the first batch from the training DataLoader\n",
    "example_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Print the shape of input IDs in the example batch\n",
    "print(\"Example batch input IDs shape:\", example_batch['input_ids'].shape)  \n",
    "\n",
    "# Print the shape of target IDs in the example batch\n",
    "print(\"Example batch target IDs shape:\", example_batch['target_ids'].shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device 'mps' to train the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████████████████████████████████████████████████| 2190/2190 [1:16:36<00:00,  2.10s/batch, loss=0.0637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1/3 --- Average Loss: 0.0935\n",
      "Saving model for Epoch 1\n",
      "Model for Epoch 1 is saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████████| 2190/2190 [1:16:34<00:00,  2.10s/batch, loss=0.0477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-2/3 --- Average Loss: 0.0759\n",
      "Saving model for Epoch 2\n",
      "Model for Epoch 2 is saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████████████████████████████████████████████████| 2190/2190 [1:16:40<00:00,  2.10s/batch, loss=0.114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-3/3 --- Average Loss: 0.0738\n",
      "Saving model for Epoch 3\n",
      "Model for Epoch 3 is saved successfully.\n",
      "All models are saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if MPS (Metal Performance Shaders) is available for GPU training in MacBook\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device '{device}' to train the model.\")\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop starts here\n",
    "num_epochs = 3\n",
    "\n",
    "# Define folder structure to create dir to save models\n",
    "output_dir = \"/Users/ravkothu/Documents/Personal_items_at_Oracle/Master_Degree/University_of_San_Diego/Online_Masters/MS_in_Applied_AI/Subjects_and_Resources/AAI-520-A2_NLP/AAI-520-A2_Final_Team_Project/Chatbot_Code/model_checkpoints\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # Add tqdm for the training loop to show progress\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for batch in train_dataloader:\n",
    "            # Move input data to GPU\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "            \n",
    "            # Get the loss from the model's outputs\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    print(f\"Epoch-{epoch + 1}/{num_epochs} --- Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save model and optimizer state after each epoch\n",
    "    print(f\"Saving model for Epoch {epoch + 1}\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': 8,\n",
    "        # Save average loss for this epoch\n",
    "        'avg_loss': avg_loss\n",
    "    }, os.path.join(output_dir, f\"epoch_{epoch + 1}_model.pt\"))\n",
    "    print(f\"Model for Epoch {epoch + 1} is saved successfully.\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'num_epochs': num_epochs,\n",
    "    'batch_size': 8,\n",
    "}, os.path.join(output_dir, \"final_model.pt\"))\n",
    "\n",
    "print(\"All models are saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if MPS (Metal Performance Shaders) is available for GPU training in Macbook\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(f\"Using device '{device}' to train the model.\")\n",
    "\n",
    "# # Load the pre-trained GPT-2 model\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# # Set the model to training mode\n",
    "# model.train()\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# # Training loop starts here\n",
    "# num_epochs = 3\n",
    "\n",
    "# # Define folder structure to create dir to save models\n",
    "# output_dir = \"/Users/ravkothu/Documents/Personal_items_at_Oracle/Master_Degree/University_of_San_Diego/Online_Masters/MS_in_Applied_AI/Subjects_and_Resources/AAI-520-A2_NLP/AAI-520-A2_Final_Team_Project/Chatbot_Code/model_checkpoints\"\n",
    "\n",
    "# # Create the directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch in train_dataloader:\n",
    "#         # Move input data to GPU\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "        \n",
    "#         # Get the loss from the model's outputs\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Update the parameters\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Accumulate the loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # Average loss for the epoch\n",
    "#     avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "#     print(f\"Epoch-{epoch + 1}/{num_epochs} --- Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "#     # Save model and optimizer state after each epoch\n",
    "#     torch.save({\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'num_epochs': num_epochs,\n",
    "#         'batch_size': 8,\n",
    "#         # Save average loss for this epoch\n",
    "#         'avg_loss': avg_loss\n",
    "#     }, os.path.join(output_dir, f\"epoch_{epoch + 1}_model.pt\"))\n",
    "\n",
    "# # Save the final model\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'num_epochs': num_epochs,\n",
    "#     'batch_size': 8,\n",
    "# }, os.path.join(output_dir, \"final_model.pt\"))\n",
    "\n",
    "# print(\"All models saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "6edc944898014100bc47705f4eed07a9",
      "cca9d9870dd248c6926ef8ca5eb71aab",
      "24a890845d9741bc9fc8f087a1796800",
      "23c82e51c5a1461a9b97d23eff838d3c",
      "9df058880cfa4d1ebc6e240bb34e7185",
      "ad25b46e09ad4f829a2bfb86220ec684",
      "aea06bdb7ddb4bcbb3ab87463d1af4ca",
      "f3957a839d3c4369a2548764875bd73e",
      "12d2e04dafa84e1faa7da02bfd1a05e6",
      "6b29dd216ee94e969f03017dacde3af0",
      "cf06284e4e3e4808a76df67d63f68a79",
      "24920818da8840f699b5e10b979e4de3",
      "e5095fea929d4904bc6cf2c8817e2ba0",
      "0272e2034ed848ed9605d8d829c0b9d3",
      "fe056a4dc5f24da8884159426878a71c",
      "33dcf44dcdd14d78a038febe7644777e",
      "7822cd6659c24711bdc30a0fd2f9d101",
      "8d88fcf3c5ae4f85bcb76c4cb7295db6",
      "bae2ec6a68904ef4bf8831d9d9c32915",
      "392b0f99d0404beea383f7b1e2dd1bee",
      "5970817755594c4e81dd679d6778acad",
      "cc65f6b4815144149f8fbe9ed7e2c61b"
     ]
    },
    "id": "d6D_YHStCGxs",
    "outputId": "33740937-eeeb-4d26-96de-3d28f04b695a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edc944898014100bc47705f4eed07a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24920818da8840f699b5e10b979e4de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "# model_name = 'gpt2'\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# # Set the pad_token to eos_token\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Resize the model embeddings to include the new pad_token\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # Move the model to GPU\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Assuming 'target_text' is your label column\n",
    "# def tokenize_function(examples):\n",
    "#     # Tokenize the input_text\n",
    "#     # print(examples['input_text'])\n",
    "#     tokenized_inputs = tokenizer(\n",
    "#         examples['input_text'],\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "#     # Tokenize the target_text\n",
    "#     tokenized_targets = tokenizer(\n",
    "#         examples['target_text'],\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=512,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "#     # Set labels\n",
    "#     tokenized_inputs[\"labels\"] = tokenized_targets[\"input_ids\"]\n",
    "\n",
    "#     return tokenized_inputs\n",
    "\n",
    "# # Apply the tokenize function\n",
    "# train_tokenized = train_dataset.map(tokenize_function, batched=False)\n",
    "# val_tokenized = val_dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "# # Set the format for PyTorch, including 'labels' instead of 'target_text'\n",
    "# train_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# val_tokenized.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RZ7a7eWH3boP"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import math\n",
    "# from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# # Assume 'full_dataset' is your complete dataset with 100,000 records\n",
    "# chunk_size = 10000\n",
    "# num_chunks = math.ceil(len(train_tokenized) / chunk_size)\n",
    "# output_dir = './fine_tuned_model'\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Define training arguments with checkpointing enabled\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     evaluation_strategy='epoch',\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=11,  # Adjust based on your GPU memory\n",
    "#     per_device_eval_batch_size=11,\n",
    "#     # Train three epochs per chunk\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "#     # Save at the end of each epoch\n",
    "#     save_strategy='epoch',\n",
    "#     # Keep only the latest checkpoint to save space\n",
    "#     save_total_limit=1,\n",
    "#     # You can change this if you prefer loading the best model\n",
    "#     load_best_model_at_end=False\n",
    "# )\n",
    "\n",
    "# # Initialize the data collator\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# # Function to check for the latest checkpoint\n",
    "# def get_latest_checkpoint(output_dir):\n",
    "#     checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")]\n",
    "#     if len(checkpoints) > 0:\n",
    "#         return max(checkpoints, key=os.path.getctime)  # Return the latest checkpoint\n",
    "#     return None\n",
    "\n",
    "# # Track which chunk to start from, in case of failure\n",
    "# chunk_start_idx = 0\n",
    "# if os.path.exists(os.path.join(output_dir, \"gpt2_training_progress.txt\")):\n",
    "#     with open(os.path.join(output_dir, \"gpt2_training_progress.txt\"), \"r\") as f:\n",
    "#         # Load the index of the chunk to resume from\n",
    "#         chunk_start_idx = int(f.read().strip())\n",
    "\n",
    "# # Loop over each chunk of the dataset\n",
    "# for i in range(chunk_start_idx, num_chunks):\n",
    "#     start_idx = i * chunk_size\n",
    "#     end_idx = min((i + 1) * chunk_size, len(train_tokenized))\n",
    "#     print(f\"Training on records {start_idx} to {end_idx - 1}\")\n",
    "\n",
    "#     # Create a subset for the current chunk\n",
    "#     train_chunk = train_tokenized.select(range(start_idx, end_idx))\n",
    "\n",
    "#     # Check if there is a saved checkpoint to load from\n",
    "#     last_checkpoint = get_latest_checkpoint(output_dir)\n",
    "\n",
    "#     # Initialize the trainer with the saved model or start fresh\n",
    "#     trainer = Trainer(\n",
    "#         # Ensure the same model is used\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_chunk,\n",
    "#         # Assuming 'val_tokenized' remains the same\n",
    "#         eval_dataset=val_tokenized,\n",
    "#         data_collator=data_collator\n",
    "#     )\n",
    "\n",
    "#     # Train from the last checkpoint if it exists, otherwise from scratch\n",
    "#     trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "#     # Save the model and tokenizer after each chunk\n",
    "#     trainer.save_model(output_dir)\n",
    "#     tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "#     # Save the progress to a text file, so we know which chunk to resume from if something fails\n",
    "#     with open(os.path.join(output_dir, \"gpt2_training_progress.txt\"), \"w\") as f:\n",
    "#         f.write(str(i + 1))  # Write the index of the next chunk to train\n",
    "\n",
    "#     # Optionally clear the CUDA cache to free memory\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Incremental training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "A_2XbE2gBexa",
    "outputId": "c9c038c4-24ed-4077-d67b-d739b6aade06"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=11,\n",
    "    per_device_eval_batch_size=11,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./fine_tuned_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q3S3mAo5Q5Zo"
   },
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "# model = GPT2LMHeadModel.from_pretrained('fine_tuned_model')\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('fine_tuned_model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0272e2034ed848ed9605d8d829c0b9d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bae2ec6a68904ef4bf8831d9d9c32915",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_392b0f99d0404beea383f7b1e2dd1bee",
      "value": 100
     }
    },
    "12d2e04dafa84e1faa7da02bfd1a05e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23c82e51c5a1461a9b97d23eff838d3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b29dd216ee94e969f03017dacde3af0",
      "placeholder": "​",
      "style": "IPY_MODEL_cf06284e4e3e4808a76df67d63f68a79",
      "value": " 100/100 [00:00&lt;00:00, 274.76 examples/s]"
     }
    },
    "24920818da8840f699b5e10b979e4de3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e5095fea929d4904bc6cf2c8817e2ba0",
       "IPY_MODEL_0272e2034ed848ed9605d8d829c0b9d3",
       "IPY_MODEL_fe056a4dc5f24da8884159426878a71c"
      ],
      "layout": "IPY_MODEL_33dcf44dcdd14d78a038febe7644777e"
     }
    },
    "24a890845d9741bc9fc8f087a1796800": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3957a839d3c4369a2548764875bd73e",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12d2e04dafa84e1faa7da02bfd1a05e6",
      "value": 100
     }
    },
    "33dcf44dcdd14d78a038febe7644777e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "392b0f99d0404beea383f7b1e2dd1bee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5970817755594c4e81dd679d6778acad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b29dd216ee94e969f03017dacde3af0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6edc944898014100bc47705f4eed07a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cca9d9870dd248c6926ef8ca5eb71aab",
       "IPY_MODEL_24a890845d9741bc9fc8f087a1796800",
       "IPY_MODEL_23c82e51c5a1461a9b97d23eff838d3c"
      ],
      "layout": "IPY_MODEL_9df058880cfa4d1ebc6e240bb34e7185"
     }
    },
    "7822cd6659c24711bdc30a0fd2f9d101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d88fcf3c5ae4f85bcb76c4cb7295db6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9df058880cfa4d1ebc6e240bb34e7185": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad25b46e09ad4f829a2bfb86220ec684": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aea06bdb7ddb4bcbb3ab87463d1af4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bae2ec6a68904ef4bf8831d9d9c32915": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc65f6b4815144149f8fbe9ed7e2c61b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cca9d9870dd248c6926ef8ca5eb71aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad25b46e09ad4f829a2bfb86220ec684",
      "placeholder": "​",
      "style": "IPY_MODEL_aea06bdb7ddb4bcbb3ab87463d1af4ca",
      "value": "Map: 100%"
     }
    },
    "cf06284e4e3e4808a76df67d63f68a79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5095fea929d4904bc6cf2c8817e2ba0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7822cd6659c24711bdc30a0fd2f9d101",
      "placeholder": "​",
      "style": "IPY_MODEL_8d88fcf3c5ae4f85bcb76c4cb7295db6",
      "value": "Map: 100%"
     }
    },
    "f3957a839d3c4369a2548764875bd73e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe056a4dc5f24da8884159426878a71c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5970817755594c4e81dd679d6778acad",
      "placeholder": "​",
      "style": "IPY_MODEL_cc65f6b4815144149f8fbe9ed7e2c61b",
      "value": " 100/100 [00:00&lt;00:00, 263.08 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
